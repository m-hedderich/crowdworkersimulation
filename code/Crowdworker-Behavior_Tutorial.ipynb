{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fd49e21",
   "metadata": {},
   "source": [
    "# A Model of Adaptive Crowdworker Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad78745",
   "metadata": {},
   "source": [
    "Tutorial on the use of the crowdworker reinforcement learning environment we provide as Supplementary Material to the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d651e2d6",
   "metadata": {},
   "source": [
    "## Installation\n",
    "If not already done, you need to install Python 3.8 and the required libraries. You can do this with conda via:\n",
    "```\n",
    "conda create -n crowdworker_env python=3.8\n",
    "conda activate crowdworker_env\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "Now we can start."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1fdd11",
   "metadata": {},
   "source": [
    "## Running a Trained Model\n",
    "We will start with an already trained model and its corresponding crowdworking environment to explore the first steps. We will start with loading the agent/crowdworker model and its environment and settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "808d6b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some imports\n",
    "import os\n",
    "from task import TaskPropertiesDistribution, AntiCheatSettings\n",
    "from util.exputil import Config\n",
    "from user import UserProperties\n",
    "from userenv import UserModelEnv\n",
    "import sb3_contrib\n",
    "\n",
    "# name of the configuration file\n",
    "# this is for a model that was trained in an environment with cheating deterrents\n",
    "# the model is stored in ../exp/[name]\n",
    "name = \"paper-exps_cheating_qa3-0.1_rep0.9\"\n",
    "\n",
    "# if you like, you can change this to \n",
    "#name = \"paper-exps_cheating_qa3-0_rep0.9\"\n",
    "# if you want to analyze a worker agent in an environment without cheating deterrents\n",
    "\n",
    "# let's load this experimental configuration (just a json file, you can actually take a look into it, if you like)\n",
    "config = Config.load(name)\n",
    "\n",
    "# now we load the worker and the environment\n",
    "\n",
    "# properties of the worker, e.g. how much he or she values monetary payout and interestigness\n",
    "user_props = UserProperties.load(config)\n",
    "\n",
    "# properties of the task-givers (here, we have 5 task-givers)\n",
    "task_prop_distributions = TaskPropertiesDistribution.load_list(config)\n",
    "\n",
    "# settings of the cheating deterrents\n",
    "anti_cheat_settings = AntiCheatSettings.load(config)\n",
    "\n",
    "# create the RL environment, which corresponds to the crowdworking platform\n",
    "env = UserModelEnv(config, user_props, task_prop_distributions, anti_cheat_settings)\n",
    "\n",
    "# loading the actual RL model (machine learning model)\n",
    "assert config.rl_model == \"QR-DQN\"\n",
    "model = sb3_contrib.qrdqn.QRDQN.load(config.path(\"model.save\"), env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b79da5",
   "metadata": {},
   "source": [
    "Now that we have loaded everything, we can let the agent work on the crowdworking platform and check what it does.\n",
    "\n",
    "We will see that the worker selects a task, works on it diligently and once it runs out of questions for the task, it switches to the next task.\n",
    "\n",
    "(If you like, you can change above the pretrained model to the one that was trained in an environment without cheating deterrents and therefore acts negligently most of the time. Just load the other model above. Note that then obviously the results change!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ce31227",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker performed action SWITCH TO TASK 3 and earned a reward of 0.00\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.59\n",
      "Worker performed action SWITCH TO TASK 4 and earned a reward of 0.00\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.76\n",
      "Worker performed action ANSWER DILIGENTLY and earned a reward of 0.00\n"
     ]
    }
   ],
   "source": [
    "# setting a seed to make results reproducible\n",
    "env.seed(98765)\n",
    "\n",
    "obs = env.reset() # a new episode\n",
    "while True:\n",
    "    # let the RL model analyze the observation (what the worker currently sees)\n",
    "    # and decide what to do (predict an action)\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    # action is an index, convert to a human-readable string representation\n",
    "    action_as_str = UserModelEnv.action_to_str(action)\n",
    "    print(f\"Worker performed action {action_as_str} and earned a reward of {reward:.2f}\")\n",
    "\n",
    "    if done: # episode has ended because worker has run out of time or has quit\n",
    "        break      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6261c5ac",
   "metadata": {},
   "source": [
    "If we want to take a closer look at what the agent can see, we can print out the observations.\n",
    "\n",
    "We can see here, that at the beginning of the episode, the agent can not see how interesting a task is or how much time effort it requires. Only the payout is visible.\n",
    "\n",
    "Once the agent tries out a task, it finds out how interesting it is and how much effort it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eb00e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At beginning of the episode:\n",
      " Observation:\n",
      "  Task 0:\n",
      "      payout 0.6253242330848118 | rounds 0.0\n",
      "      expert -1.0 | effort -1.0 | interest -1.0\n",
      "  Task 1:\n",
      "      payout 0.33753537837799463 | rounds 0.0\n",
      "      expert -1.0 | effort -1.0 | interest -1.0\n",
      "  Task 2:\n",
      "      payout 0.5568837901272194 | rounds 0.0\n",
      "      expert -1.0 | effort -1.0 | interest -1.0\n",
      "  Task 3:\n",
      "      payout 0.6333947080897941 | rounds 0.0\n",
      "      expert -1.0 | effort -1.0 | interest -1.0\n",
      "  Task 4:\n",
      "      payout 0.6768648101640447 | rounds 0.0\n",
      "      expert -1.0 | effort -1.0 | interest -1.0\n",
      "  current task: -1.0\n",
      "  reputation: 1.0\n",
      "  time: 0.0/50.0\n",
      "\n",
      "After trying out Task 1:\n",
      " Observation:\n",
      "  Task 0:\n",
      "      payout 0.6253242330848118 | rounds 0.0\n",
      "      expert -1.0 | effort -1.0 | interest -1.0\n",
      "  Task 1:\n",
      "      payout 0.33753537837799463 | rounds 1.0\n",
      "      expert 0.8589561657442589 | effort 0.5247141825974938 | interest 0.037395870193654535\n",
      "  Task 2:\n",
      "      payout 0.5568837901272194 | rounds 0.0\n",
      "      expert -1.0 | effort -1.0 | interest -1.0\n",
      "  Task 3:\n",
      "      payout 0.6333947080897941 | rounds 0.0\n",
      "      expert -1.0 | effort -1.0 | interest -1.0\n",
      "  Task 4:\n",
      "      payout 0.6768648101640447 | rounds 0.0\n",
      "      expert -1.0 | effort -1.0 | interest -1.0\n",
      "  current task: 1.0\n",
      "  reputation: 1.0\n",
      "  time: 1.524714182597494/50.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setting a seed to make results reproducible\n",
    "env.seed(98765)\n",
    "\n",
    "# a new episode\n",
    "obs = env.reset() \n",
    "\n",
    "print(f\"At beginning of the episode:\\n {env.observation_to_string(obs)}\")\n",
    "\n",
    "# let's take some manual steps in the environment (instead of the trained RL model)\n",
    "env.step(UserModelEnv.SWITCH_TASK0+1) # select task 1\n",
    "obs, reward, done, info  = env.step(UserModelEnv.ACTION_ANS_INTENT) # answer diligently\n",
    "\n",
    "print(f\"After trying out Task 1:\\n {env.observation_to_string(obs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c24fe36",
   "metadata": {},
   "source": [
    "We can inspect the behavior of the agent more in depth. Here, we analyze for which task-giver the agent works. Since the properties of the tasks are beta-distributed (so in each episode they are somewhat different), we repeat the experiment multiple times and take the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abb356c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent answered on average 85.39 questions per episode.\n",
      "It answered on average 17.46 questions for task-giver 0.\n",
      "This means, on average 0.21880928415930798 of the questions were for task-giver 0.\n",
      "In the long run, with 5 task-givers, one would expect a ratio of 0.2 questions.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "\n",
    "# setting a seed to make results reproducible\n",
    "env.seed(98765)\n",
    "\n",
    "# 100 episodes or repetitions \n",
    "# (in the paper, we always use 1000 repetitions, but that takes a bit more time)\n",
    "repetitions = 100\n",
    "\n",
    "# counting how many questions answered in total and for task_giver_0\n",
    "# list of counters for all repetitions\n",
    "# we use lists so that we can compute the mean over all episodes\n",
    "counters_for_all_task_givers = []\n",
    "counters_for_task_giver_0 = []\n",
    "\n",
    "for i in range(repetitions): \n",
    "    # a new episode\n",
    "    obs = env.reset() \n",
    "    \n",
    "    # reset counters for this episode\n",
    "    counter_for_all_task_givers = 0\n",
    "    counter_for_task_giver_0 = 0\n",
    "    \n",
    "    while True:\n",
    "        # let the RL model analyze the observation (what the worker currently sees)\n",
    "        # and decide what to do (predict an action)\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        # if the worker answered a question / the agent did an answering action\n",
    "        if action == UserModelEnv.ACTION_ANS_INTENT or action == UserModelEnv.ACTION_ANS_RND:\n",
    "            counter_for_all_task_givers += 1\n",
    "\n",
    "            if env.current_task_idx != -1: # should not happen, but just in case the agent tries to answer a question before selecting a task\n",
    "\n",
    "                # the list of tasks is randomized at the beginning of every episode so that the agent\n",
    "                # can not learn a connection between task-givers and tasks (for the case that task-givers\n",
    "                # have different distributions). With this map, we can get the task-giver for a task.\n",
    "                task_giver_for_current_task = env.task_task_dist_map[env.current_task_idx]\n",
    "\n",
    "                if task_giver_for_current_task == 0:\n",
    "                    counter_for_task_giver_0 += 1\n",
    "\n",
    "        if done: # episode has ended because worker has run out of time or has quit\n",
    "\n",
    "            # add the measurements to the list of counters\n",
    "            counters_for_all_task_givers.append(counter_for_all_task_givers)\n",
    "            counters_for_task_giver_0.append(counter_for_task_giver_0)\n",
    "\n",
    "            break      \n",
    "\n",
    "ratio_task_giver_0 = np.mean([i/j for i,j in zip(counters_for_task_giver_0, counters_for_all_task_givers)])\n",
    "print(f\"The agent answered on average {np.mean(counters_for_all_task_givers)} questions per episode.\")\n",
    "print(f\"It answered on average {np.mean(counters_for_task_giver_0)} questions for task-giver 0.\")\n",
    "print(f\"This means, on average {ratio_task_giver_0} of the questions were for task-giver 0.\")\n",
    "print(f\"In the long run, with 5 task-givers, one would expect a ratio of {1/5} questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aec9cb4",
   "metadata": {},
   "source": [
    "## Changing the Environment and Analyzing Effects\n",
    "\n",
    "We can change the crowdworking environment and see how this affects the behavior of the worker agent.\n",
    "\n",
    "If you change the environment substantionally, you will have to retrain the RL model, so that it can learn how to work with these changes. We will see how to do this retraining below. Here, however, we only do a small change, so we can still use the pre-trained environment.\n",
    "\n",
    "We will create a new environment where task-giver 0 has a higher payout. In the paper, we saw that an increased payout motivates the worker to work more on tasks by this task-giver. We see the same effect replicated here. For more information on the effect, take a look at the paper, Section 5, effect A.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "193e9372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent answered on on average 0.5027607153753726 of the questions for task-giver 0.\n",
      "This is much higher than the ratio we saw above when all task-givers were equal.\n"
     ]
    }
   ],
   "source": [
    "# Very similar code to above. The only change is the modified task_prop_distributions \n",
    "# (i.e. the properties of the task-givers)\n",
    "\n",
    "# preparing the environment and loading the RL model\n",
    "from task import TaskPropertiesCustomFixedDistribution, TaskPropertiesCustomBetaDistribution, TaskPropertiesBetaDistribution\n",
    "\n",
    "name = \"paper-exps_cheating_qa3-0.1_rep0.9\"\n",
    "config = Config.load(name)\n",
    "user_props = UserProperties.load(config)\n",
    "\n",
    "# properties of the task-givers\n",
    "# instead of loading task-property-distributions (task-givers),\n",
    "# here we create new ones. The first task-giver (task-giver 0) has a higher payout.\n",
    "# you can take a look at task.py to see how you can change the task-giver distributions.\n",
    "# you can also use TaskPropertiesCustomFixedDistribution if you want fixed values instead of distributions.\n",
    "\n",
    "# task-giver 0 has a shifted beta distribution, the other task-givers stay the same.\n",
    "task_prop_distributions = [TaskPropertiesCustomBetaDistribution(payout=(90, 10)),\n",
    "                          TaskPropertiesBetaDistribution(),\n",
    "                          TaskPropertiesBetaDistribution(),\n",
    "                          TaskPropertiesBetaDistribution(),\n",
    "                          TaskPropertiesBetaDistribution()]\n",
    "\n",
    "# the other settings are loaded again\n",
    "anti_cheat_settings = AntiCheatSettings.load(config)\n",
    "env = UserModelEnv(config, user_props, task_prop_distributions, anti_cheat_settings)\n",
    "assert config.rl_model == \"QR-DQN\"\n",
    "model = sb3_contrib.qrdqn.QRDQN.load(config.path(\"model.save\"), env=env)\n",
    "\n",
    "# now we run the simulation and analyze how the agent reacts\n",
    "# the rest of the code is identical to the commented version above\n",
    "\n",
    "env.seed(98765)\n",
    "repetitions = 100\n",
    "counters_for_all_task_givers = []\n",
    "counters_for_task_giver_0 = []\n",
    "\n",
    "for i in range(repetitions): \n",
    "    obs = env.reset() \n",
    "    \n",
    "    counter_for_all_task_givers = 0\n",
    "    counter_for_task_giver_0 = 0\n",
    "    \n",
    "    while True:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        if action == UserModelEnv.ACTION_ANS_INTENT or action == UserModelEnv.ACTION_ANS_RND:\n",
    "            counter_for_all_task_givers += 1\n",
    "\n",
    "            if env.current_task_idx != -1: \n",
    "                task_giver_for_current_task = env.task_task_dist_map[env.current_task_idx]\n",
    "\n",
    "                if task_giver_for_current_task == 0:\n",
    "                    counter_for_task_giver_0 += 1\n",
    "\n",
    "        if done:\n",
    "            counters_for_all_task_givers.append(counter_for_all_task_givers)\n",
    "            counters_for_task_giver_0.append(counter_for_task_giver_0)\n",
    "\n",
    "            break      \n",
    "\n",
    "ratio_task_giver_0 = np.mean([i/j for i,j in zip(counters_for_task_giver_0, counters_for_all_task_givers)])\n",
    "print(f\"The agent answered on on average {ratio_task_giver_0} of the questions for task-giver 0.\")\n",
    "print(f\"This is much higher than the ratio we saw above when all task-givers were equal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0194bd",
   "metadata": {},
   "source": [
    "## Training a New Model\n",
    "If you change the crowdworking environment in a larger way, you need to train a new RL model so that it can learn what the optimal behavior in this new setting is. Here, we show how to perform the training.\n",
    "\n",
    "While the RL model can be trained on a CPU, the training of the deep neural network is much faster if you have a GPU. For this tutorial, we only train the RL model for a few steps so that you can see how the code for the training works. You can of course perform a long training and evaluate your own trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ef5c162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'tutorial_model_no-cheating-deterrent', 'timestamp': '20:15PM CEST on Sep 20, 2022', 'exp_dir_path': '../exp/tutorial_model_no-cheating-deterrent', 'description': 'A model build in the tutorial without cheating deterrents.', 'rl_model': 'QR-DQN', 'exploration_fraction': 0.2, 'exploration_final_eps': 0.05, 'learning_starts': 500, 'total_timesteps': 10, 'main_seed': 12345}\n",
      "Logging to ../exp/tutorial_model_no-cheating-deterrent\n",
      "Training took 0.0022253990173339844 seconds.\n",
      "This was not a full training as we only trained for 10 steps!\n",
      "Increase config.total_timesteps for an actual training.\n",
      "You can load this model by using the name 'tutorial_model_no-cheating-deterrent'\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from rllearning import rl_training\n",
    "\n",
    "# the name of your setting, a directory will be created in ../exp/[name]\n",
    "name = f\"tutorial_model_no-cheating-deterrent\"\n",
    "\n",
    "config = Config.create(name, exist_ok=True)\n",
    "config.description = \"A model build in the tutorial without cheating deterrents.\"\n",
    "\n",
    "# properties of the worker (how much they care for the interestingness and the payout reward,\n",
    "# how large their time budget is and the reputation level they start with)\n",
    "user_props = UserProperties(interestingness_sensitivity=0.5, payout_sensitivity=1,\n",
    "                            time_sensitivity=0, time_budget=50, start_reputation=1)\n",
    "\n",
    "# default task-giver property distributions\n",
    "task_prop_distributions = [TaskPropertiesBetaDistribution()] * 5\n",
    "\n",
    "# cheating deterrents\n",
    "ban_after = 3 # number of gold questions answered incorrectly after which the worker is banned from the task\n",
    "qa_prct = 0 # probability of a hidden gold question\n",
    "reputation_delta = 0.05 # how the reputation changes with a correct or incorrect gold question\n",
    "reputation_lvl = 0 # minimum reputation level\n",
    "anti_cheat_settings = AntiCheatSettings(ban_after, qa_prct, -reputation_delta, +reputation_delta, reputation_lvl)\n",
    "\n",
    "# hyperparemters of the deep RL training\n",
    "config.rl_model = \"QR-DQN\"\n",
    "config.exploration_fraction = 0.2\n",
    "config.exploration_final_eps = 0.05\n",
    "config.learning_starts = 500\n",
    "\n",
    "# the number of training steps\n",
    "config.total_timesteps = 10 \n",
    "# for the tutorial, set to 10, just to see that it works\n",
    "# if you want to actually train the model, use \n",
    "#config.total_timesteps = 10000000\n",
    "\n",
    "# seed for reproducable trainings\n",
    "config.main_seed = 12345\n",
    "\n",
    "# save all the settings everything\n",
    "config.save()\n",
    "user_props.save(config)\n",
    "TaskPropertiesBetaDistribution.save_list(task_prop_distributions, config)\n",
    "anti_cheat_settings.save(config)\n",
    "\n",
    "rl_training(config, user_props, task_prop_distributions, anti_cheat_settings)\n",
    "print(f\"This was not a full training as we only trained for {config.total_timesteps} steps!\")\n",
    "print(\"Increase config.total_timesteps for an actual training.\")\n",
    "\n",
    "print(f\"You can load this model by using the name '{config.name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b754ba82",
   "metadata": {},
   "source": [
    "Now you should have seen everything you need to start diving into simulating crowdworker behavior. You can modify the settings and see how this affects the behavior of the crowdworker. You can find further information on the specific classes in the code's documentation.\n",
    "\n",
    "If you have any questions or issues, feel free to reach out to us!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
